# 数据侧写

由题意可知，本题目为典型的二分类问题，因此提前对数据进行侧写，从宏观上把握数据的整体分布、摸清各属性之间的联系和特征有利于分类精确度的提升。由于附件中所提供的各表包含的信息与最终用户是否购买情况息息相关，本节将所提供的数据进行总体观察。使用 `pandas_profiling` 和 `pandasgui` 库对附件所提供数据的分布情况进行总体分析结果如下

同时为了检查数据完整情况，使用seaborn绘制数据缺失分布图,其中含有缺失数据的数据集展示如下：

（放部分图片）

为展示各变量之间的相关性关系，使用python绘制相关性散点图如下：




# 数据汇总

因为要根据用户的各种行为和属性预测最终的购买意愿，用户的信息维度需要尽量全面，同时在数据侧写中可以观察到各表中用户id大致可以一一对应。

因此以用户id作为目标匹配属性，将login_day,user_info,visit_info三个表进行连接，并添加result表中用户的最终购买属性，令购买用户属性值为1，其余未购买用户属性值为0，合并成总表datamerge.csv。

使用seaborn绘制合并后的数据缺失分布图如下所示，可以发现各表之间缺失数据较少，有汇总的价值。

（放总图）

# 数据解读

详细分析数据的规律，我们对数据源可以得出以下模型假设：

1、用户均在中国境内，且其基本属性、用户行为与是否购买之间的关系具有一致性，可以用单一模型（组）统一预测，不需要针对性建模。

2、登录间隔“-1”值，含义为“注册后从未登录的不活跃用户”

3、由于大量登陆时长数据超过24，因此认为登陆时长单位为分钟

4、由于大量年龄值超过人类平均年龄，且该属性数值除以12后，大多分布在普遍受教育阶段，因此认为年龄属性单位为月。

5、由于存在少数“访问数、领劵数量”大大超出正常值的用户，推测该数据集内可能含有刷单用户或管理员测试用账户。在后续操作中需要进一步筛查。

6、距离期末时间含有负值，推测是用户在采集数据时间节点过后再次产生数据导致，非异常值。

# 缺失值处理

根据缺失值图，可以发现20.7%的用户缺失了“[`city_num`](#pp_var_-3484386243083903475)”属性，为保证数据的全面性，对此类用户属性统一补充为“未知”

同时极少量用户缺失了大部分的属性，由于占比大约为0.9%，且丢失属性过多，不具备预测的价值，因此这部分数据可以直接丢弃。保险起见，计算待丢弃用户的购买期望值，并与总体期望值对比，发现大致符合，即该类用户非特殊群体，可以放心删除。

# 异常值处理

对各个维度数据绘制箱线图，针对其中的长尾数据进行分析，发现以下数据存在长尾分布

（图）

以下数据有异常值

（图）

因此在可视化时，需要对具有明显异常值的属性进行筛选，对于难以区分特殊值和异常值的属性，需要进一步做分箱化处理，其中将过于超出正常范围的个体归入“非正常”类别。

而对于模型预测来说，如果使用基于决策树的模型，具有长尾分布的属性中，特殊数据也是显著的特征，盲目丢弃或分箱会造成精度下降，因此对于用来预测数据中的异常值不作清洗。

# 冗余属性处理

根据平台总览结果，app_num结果为全1，即所有用户都激活了app，因此该属性和最终是否购买之间没有任何关联，对可视化和模型训练均无贡献，需要删除。同样，click_buy属性经过异常值处理后也为全0，需要删除。

# 编码调整

根据总览结果可以发现，city_num属性为文本数据，first_order_time属性为时间序列数据，均难以直接用于模型预测，因此将city_num属性转化为onehot编码，first_order_time转化为时间戳，即相对于格林威治时间1970年01月01日00时00分00秒，北京时间1970年01月01日08时00分00秒的总秒数。

# 数据平衡

根据总览结果，最终购买数的用户和未购买用户比例为28比1，属于严重偏态数据。在这种情况下，即使全部预测为不购买，也有96.6%的准确率，因此使用合成少数类过采样技术进行平衡。

其基本思想是通过对少数类样本进行散布分析，并根据分布规律人工合成新样本数据添加到数据集中，以达到平衡分布的目的。因为主要使用KNN的方法合成新样本，而不是简单的进行随机复制，因此其结果更具有代表性。

其算法伪代码表示为：

```
当数据集中数据不满足设定条件时：
	随机取数据集中的n个少类样本
	对于n个目标点中的每个点：
        使用KNN方法分别找出离该点最近的m个少类样本
		在这m个少类样本中任取一个点，与该点进行随机线性插值
	将所生成的点加入原数据集
```

为增加随机性，使训练的模型更具有普适性，并保证测试集的数据均为真实数据，本文中所有数据平衡的地方均只针对训练集做处理，且均在训练前生成随机数据。

